{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebfc3ca1",
   "metadata": {},
   "source": [
    "THIS IS THE NOTEBEOOK WHERE WE ARE GOING TO FIRST INITIALIZE A MODEL AND THEN THEN EMBEDD THE DATA INTO THE RAG PIPELINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08895a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load the FIFA matches data\n",
    "df = pd.read_csv(\"data/Fifa_world_cup_matches.csv\")  # adjust path if needed\n",
    "\n",
    "# Quick peek\n",
    "print(df.head())\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f743500",
   "metadata": {},
   "source": [
    "converting each row of the data into text to embedded into the vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0730b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_text(row):\n",
    "    # Adjust column names to your actual CSV headers\n",
    "    return (\n",
    "       \n",
    "        f\"FIFA World Cup 2022 Match {row['team1']} vs {row['team2']}. \"\n",
    "        f\"Score: {row['number of goals team1']} - {row['number of goals team2']}. \"\n",
    "        f\"Date: {row['date']}. \"\n",
    "        f\"Category:{row['category']} ,\"\n",
    "        f\"Possesion of {row['team1']} is {row['possession team1']} and {row['team2']} is {row['possession team2']} \"\n",
    "        f\"{row['team1']} had {row['on target attempts team1']} On Target Attempts and {row['team2']} had  {row['on target attempts team2']} attempts\"\n",
    "         f\"{row['team1']} had {row['assists team1']} assists and {row['team2']} had {row['assists team2']} assists\"\n",
    "    )\n",
    "\n",
    "docs = []\n",
    "for _, row in df.iterrows():\n",
    "    text = row_to_text(row)\n",
    "    metadata = {\n",
    "        \"team1\": row[\"team1\"],\n",
    "        \"team2\": row[\"team2\"],\n",
    "        \"category\":row['category']\n",
    "    }\n",
    "    docs.append(Document(page_content=text, metadata=metadata))\n",
    "\n",
    "len(docs), docs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4384254",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41238700",
   "metadata": {},
   "source": [
    "INTIATING THE EMBEDDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6105b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c846e",
   "metadata": {},
   "source": [
    "TEXT SPLITTER FOR THE RECAPS DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2119469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def store_agent_responses_with_chunking(responses_dict, model_name=\"llama3\"):\n",
    "    \"\"\"\n",
    "    Store agent responses in a FAISS vector store using Ollama embeddings\n",
    "    with recursive text splitting for better chunk management\n",
    "    \n",
    "    Args:\n",
    "        responses_dict: Dictionary with response names as keys and response content as values\n",
    "        model_name: Ollama model to use for embeddings (default: llama3)\n",
    "    \n",
    "    Returns:\n",
    "        vector_store: FAISS vector store object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize Ollama embeddings\n",
    "    embeddings = OllamaEmbeddings(model=model_name)\n",
    "    \n",
    "    # Initialize recursive character text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,           # Size of each chunk\n",
    "        chunk_overlap=100,        # Overlap between chunks to maintain context\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Split on these delimiters in order\n",
    "    )\n",
    "    \n",
    "    # Create Document objects and chunk them\n",
    "    docs = []\n",
    "    for response_name, response_content in responses_dict.items():\n",
    "        # Split the text into chunks\n",
    "        chunks = text_splitter.split_text(response_content)\n",
    "        \n",
    "        # Create documents from chunks\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"response_type\": response_name,\n",
    "                    \"source\": \"agent_response\",\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(chunks)\n",
    "                }\n",
    "            )\n",
    "            docs.append(doc)\n",
    "    \n",
    "    # Create FAISS vector store\n",
    "    vector_store = FAISS.from_documents(docs, embedding=embeddings)\n",
    "    \n",
    "    print(f\"✅ Successfully stored {len(docs)} chunks from {len(responses_dict)} responses!\")\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "# Usage example\n",
    "responses = {\n",
    "    \"portugal_vs_morocco\": result_3[\"messages\"][-1].content,\n",
    "    \"possession_stats\": result_2[\"messages\"][-1].content,\n",
    "    \"uruguay_performance\": resulted[\"messages\"][-1].content,\n",
    "}\n",
    "\n",
    "# Create vector store with chunking\n",
    "response_vector_store = store_agent_responses_with_chunking(responses)\n",
    "\n",
    "# Create retriever\n",
    "response_retriever = response_vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "# Test searching through chunked responses\n",
    "search_results = response_retriever.invoke(\"possession statistics\")\n",
    "for doc in search_results:\n",
    "    print(f\"Response: {doc.metadata['response_type']} (chunk {doc.metadata['chunk_index']})\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(docs, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67adea9",
   "metadata": {},
   "source": [
    "Now that the vector embeddings are completed we will now be doing a test for similarity search so that we can see what is actually going on here now using similiratiy_score_threshhold\n",
    "\n",
    "similarity = “just give me closest stuff”\n",
    "\n",
    "similarity_score_threshold = “closest stuff but drop trash”\n",
    "\n",
    "mmr = “closest but also diverse (good for longer answers)”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"k\": 10,                 # consider up to 10\n",
    "        \"score_threshold\": 0.9   # filter out low similarity\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_s = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",               # or \"similarity_score_threshold\", \"mmr\"\n",
    "    search_kwargs={\"k\": 5}                  # top 5 matches\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c25a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me about the match between Portugal and Morocco\"  # ⬅️ Add this\n",
    "\n",
    "results = retriever_s.invoke(prompt)   # ⬅️ this replaces get_relevant_documents\n",
    "\n",
    "for doc in results:\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e610106",
   "metadata": {},
   "source": [
    "Now Setting Up The Actualy LLM and then seeing what can be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain.tools import tool,ToolRuntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f968f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:3b\",  # or any local ollama model name you’ve pulled\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "checkpointer = InMemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(retriever_s,name='retriever_data',description='Usign this tool we will retrive the data from the vector store')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b34307",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_data(query: str) -> str:\n",
    "    \"\"\"Retrieve World Cup match data from the vector store\"\"\"\n",
    "    results = retriever_s.invoke(query)\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[retrieve_data],\n",
    "    checkpointer=checkpointer,\n",
    "    system_prompt=\"\"\"You are a World Cup football data assistant.\n",
    "First call retrieve_data to retrieve context.\n",
    "Use ONLY the data from the retrieve_data tool to answer the question.\n",
    "If the data does not contain the answer, say so clearly.\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122905a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\": {\"thread_id\": \"9\"}}\n",
    "\n",
    "result=agent.invoke({\n",
    "    \"messages\":[{\"role\":\"user\",\"content\":\"Hey I am Ananda Prem How are you doing\"}]\n",
    "},config)\n",
    "\n",
    "resulted=agent.invoke({\n",
    "    \"messages\":[{\"role\":\"user\",\"content\":\"How Did Urugay Perform in this World Cup\"}]\n",
    "},config)\n",
    "\n",
    "result_2=agent.invoke({\n",
    "    \"messages\":[{\"role\":\"user\",\"content\":\"What would you say about the key moment where their fate in the worldcup was decided\"}]\n",
    "},config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9c253",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Result 1:\")\n",
    "print(resulted[\"messages\"][-1].content)\n",
    "\n",
    "print(\"\\nResult 2:\")\n",
    "print(result_2[\"messages\"][-1].content)\n",
    "\n",
    "print(\"\\nResult 3:\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494278e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Result 3 (Full):\")\n",
    "print(result_3[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab9222",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"messages\"][-1].pretty_print()\n",
    "result_2[\"messages\"][-1].pretty_print()\n",
    "result_3[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "agent.invoke({\"messages\": \"hi, my name is Nathan\"}, config)\n",
    "resp=agent.invoke({\"messages\": \"Tell me about Portugals Match Against Morroco\"}, config)\n",
    "resp=agent.invoke({\"messages\": \"What Would You tell About the possession did portugal deserve to win based off the possesion they maintained throught the match\"}, config)\n",
    "resp[\"messages\"][-1].pretty_print()\n",
    "resp=agent.invoke({\"messages\":\"What is my name again ?\"},config)\n",
    "resp[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf162b",
   "metadata": {},
   "source": [
    "Creating a Function to Get Input from a User instead Of hardCoding it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".match",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
